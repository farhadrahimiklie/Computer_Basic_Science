# ğŸ”¥ PART 1 â€” Why Cache Exists

CPU cores are insanely fast.

Modern CPUs:

* ~3 GHz â†’ 3 **billion** cycles/sec
* One cycle â‰ˆ 0.3 ns

Main memory (DRAM):

* ~80â€“150 ns latency

Thatâ€™s **250â€“500 CPU cycles** just waiting.

Without cache:
CPU would spend most of its life **doing nothing**.

---

### Memory Hierarchy

```
Registers   (1 cycle)
L1 Cache    (~3-5 cycles)
L2 Cache    (~10-15)
L3 Cache    (~30-60)
RAM         (~100+)
Disk        (millions)
```

Closer = faster + smaller + expensive.

---

# ğŸ”¥ PART 2 â€” What Is Cache?

Cache is:

> **Small, very fast memory that stores recently used memory blocks.**

Not bytes.
**BLOCKS.** Called **cache lines**.

Typical cache line:

```
64 bytes
```

If CPU loads:

```
int x = a[100];
```

It doesn't fetch just that int.

It fetches:
â¡ï¸ the **entire 64â€‘byte line** containing it.

This is because programs usually exhibit:

### 1ï¸âƒ£ Temporal Locality

If you used it now â†’ you'll likely use it again soon.

### 2ï¸âƒ£ Spatial Locality

If you used address X â†’ youâ€™ll soon use X+1, X+2â€¦

---

# ğŸ”¥ PART 3 â€” Cache Structure

A cache is NOT an array of bytes.

It is:

```
[ TAG | DATA BLOCK | VALID | DIRTY ]
```

Each entry:

* **TAG** â†’ which memory block this is
* **DATA** â†’ 64 bytes
* **VALID** â†’ contains real data?
* **DIRTY** â†’ modified?

---

# ğŸ”¥ PART 4 â€” Address Breakdown

A 64â€‘bit memory address is split:

```
| TAG | INDEX | OFFSET |
```

* OFFSET â†’ which byte inside 64â€‘byte line (6 bits)
* INDEX â†’ which set
* TAG â†’ identifies the block

---

Example:

Cache:

* 32 KB L1
* 64B line
* 8â€‘way associative

Lines:

```
32 KB / 64 B = 512 lines
```

Sets:

```
512 / 8 = 64 sets
```

Index bits:

```
log2(64) = 6 bits
```

Offset:

```
log2(64) = 6 bits
```

Remaining bits â†’ TAG.

---

# ğŸ”¥ PART 5 â€” Mapping Types

How memory maps to cache.

---

## 1ï¸âƒ£ Directâ€‘Mapped

Each block â†’ exactly one location.

Fast.
But collisions are brutal.

Two addresses that map to same index constantly evict each other.

---

## 2ï¸âƒ£ Fully Associative

Any block anywhere.

Very flexible.
Expensive hardware.
Rare for large caches.

---

## 3ï¸âƒ£ Setâ€‘Associative (most common)

Each set has N ways.

8â€‘way = 8 possible places per index.

---

# ğŸ”¥ PART 6 â€” Replacement Policies

When set full, who gets kicked?

* **LRU** â€” least recently used
* **Pseudoâ€‘LRU**
* **Random**
* **FIFO**

Real CPUs approximate LRU.

---

# ğŸ”¥ PART 7 â€” Write Policies

When CPU writes:

---

## Writeâ€‘Through

Immediately write to RAM.

Simple.
Slow.

---

## Writeâ€‘Back

Only update cache.
Mark line DIRTY.
Write back later when evicted.

Modern CPUs use this.

---

## Write Allocate

On write miss â†’ load block then write.

## No Write Allocate

Write straight to memory.

---

# ğŸ”¥ PART 8 â€” Multicore = Cache Coherence

Each core has its own L1/L2.

What if two cores touch same variable?

ğŸ’¥ Inconsistency.

Solved via **MESI protocol**:

States:

* **M**odified
* **E**xclusive
* **S**hared
* **I**nvalid

Hardware snoops buses to keep caches synchronized.

---

# ğŸ”¥ PART 9 â€” False Sharing (VERY IMPORTANT)

Two threads:

```
struct Data {
    int a;
    int b;
};
```

Thread 1 writes `a`.
Thread 2 writes `b`.

Same cache line â†’ cores invalidate each other constantly.

Even though variables differ.

This DESTROYS performance.

---

Fix:

```
struct Data {
    int a;
    char pad[60];
    int b;
};
```

Force into different cache lines.

---

# ğŸ”¥ PART 10 â€” Cache Miss Types

* **Cold miss** â€” first access
* **Capacity miss** â€” cache too small
* **Conflict miss** â€” mapping collisions

---

# ğŸ”¥ PART 11 â€” Prefetching

CPU guesses future accesses and loads early.

Also manual:

```c
__builtin_prefetch(ptr + 64);
```

GCC/Clang.

---

# ğŸ”¥ PART 12 â€” TLB â‰  Cache

TLB caches **page table translations**, not data.

Virtual â†’ Physical mapping.

TLB miss = expensive.

Huge pages help.

---

# ğŸ”¥ PART 13 â€” Measuring Cache Behavior

Simple benchmark:

```c
#include <stdio.h>
#include <stdlib.h>
#include <time.h>

#define N (1024 * 1024 * 64)

int main() {
    int *arr = malloc(sizeof(int) * N);

    volatile long long sum = 0;

    clock_t start = clock();

    for (long long i = 0; i < N; i++)
        sum += arr[i];

    clock_t end = clock();

    printf("Time: %f\n",
           (double)(end - start) / CLOCKS_PER_SEC);

    free(arr);
}
```

Change stride:

```c
for (long long i = 0; i < N; i += 16)
```

Watch speed collapse once exceeding cache.

---

# ğŸ”¥ PART 14 â€” Cacheâ€‘Friendly Programming

---

## âŒ BAD

```c
int a[4096][4096];

for (int j = 0; j < 4096; j++)
    for (int i = 0; i < 4096; i++)
        sum += a[i][j];
```

Columnâ€‘wise â†’ stride of 4096 ints.

Cache miss city.

---

## âœ… GOOD

```c
for (int i = 0; i < 4096; i++)
    for (int j = 0; j < 4096; j++)
        sum += a[i][j];
```

Rowâ€‘wise contiguous.

---

---

# ğŸ”¥ PART 15 â€” Blocking / Tiling

For matrix multiply:

---

## âŒ Naive

```c
for (int i = 0; i < N; i++)
  for (int j = 0; j < N; j++)
    for (int k = 0; k < N; k++)
      C[i][j] += A[i][k] * B[k][j];
```

---

## âœ… Cacheâ€‘Blocked

```c
#define BS 64

for (int ii = 0; ii < N; ii += BS)
  for (int jj = 0; jj < N; jj += BS)
    for (int kk = 0; kk < N; kk += BS)

      for (int i = ii; i < ii + BS; i++)
        for (int j = jj; j < jj + BS; j++)
          for (int k = kk; k < kk + BS; k++)
            C[i][j] += A[i][k] * B[k][j];
```

This is **real systemsâ€‘level optimization**.

---

# ğŸ”¥ PART 16 â€” How to THINK Like Cache

When writing code ask:

âœ” Is memory contiguous?
âœ” Am I reusing data soon?
âœ” Do threads touch same lines?
âœ” Is working set bigger than L1/L2?
âœ” Are strides multiples of cache size?
âœ” Am I thrashing sets?

---

---

# ğŸ¯ MASTER LEVEL SUMMARY

Cache is:

â€¢ Hardwareâ€‘managed
â€¢ Lineâ€‘based
â€¢ Setâ€‘associative
â€¢ Coherent
â€¢ Speculative (prefetching)
â€¢ Writeâ€‘back
â€¢ Critical to performance

If you understand cache, you understand:

â¡ï¸ why databases design pages
â¡ï¸ why games use SoA layouts
â¡ï¸ why compilers reorder loops
â¡ï¸ why parallel programs stall
â¡ï¸ why NUMA matters
â¡ï¸ why HPC exists



